import gradio as gr
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import numpy as np
import cv2
import json
from datetime import datetime

# GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
device = 0 if torch.cuda.is_available() else -1
print(f"ğŸš€ Using device: {'GPU' if device == 0 else 'CPU'}")

# æœ€æ–°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("ğŸ”„ Loading AI models...")

# 1. ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ï¼ˆDETR - æœ€æ–°ã®Transformerãƒ™ãƒ¼ã‚¹æ¤œå‡ºå™¨ï¼‰
try:
    object_detector = pipeline(
        "object-detection",
        model="facebook/detr-resnet-50",
        device=device
    )
    print("âœ… Object detection model loaded (DETR)")
except Exception as e:
    print(f"âš ï¸ Object detection model loading failed: {e}")
    object_detector = None

# 2. ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«
try:
    image_classifier = pipeline(
        "image-classification",
        model="google/vit-base-patch16-224",
        device=device
    )
    print("âœ… Image classification model loaded (ViT)")
except Exception as e:
    print(f"âš ï¸ Image classifier loading failed: {e}")
    image_classifier = None

# 3. ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®é–¢ä¿‚æ€§ã‚’ç†è§£ï¼‰
try:
    zero_shot_classifier = pipeline(
        "zero-shot-classification",
        model="facebook/bart-large-mnli",
        device=device
    )
    print("âœ… Zero-shot classifier loaded")
except Exception as e:
    print(f"âš ï¸ Zero-shot classifier loading failed: {e}")
    zero_shot_classifier = None

# ===== ç‰©ä½“æ¤œå‡ºé–¢é€£ã®é–¢æ•° =====
def detect_objects(image, confidence_threshold):
    """ç”»åƒã‹ã‚‰ç‰©ä½“ã‚’æ¤œå‡ºã—ã¦ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»"""
    if object_detector is None:
        return image, "âš ï¸ ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    
    if image is None:
        return None, "ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    
    try:
        # ç‰©ä½“æ¤œå‡ºã‚’å®Ÿè¡Œ
        results = object_detector(image, threshold=confidence_threshold)
        
        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼ã—ã¦æç”»
        img_with_boxes = image.copy()
        draw = ImageDraw.Draw(img_with_boxes)
        
        # æ¤œå‡ºçµæœã®ãƒªã‚¹ãƒˆ
        detections = []
        
        # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255),
            (255, 255, 0), (255, 0, 255), (0, 255, 255)
        ]
        
        for idx, detection in enumerate(results):
            box = detection['box']
            label = detection['label']
            score = detection['score']
            
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»
            x1, y1 = box['xmin'], box['ymin']
            x2, y2 = box['xmax'], box['ymax']
            
            color = colors[idx % len(colors)]
            
            # ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»
            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
            
            # ãƒ©ãƒ™ãƒ«ã‚’æç”»
            text = f"{label}: {score:.2f}"
            draw.text((x1, y1 - 15), text, fill=color)
            
            detections.append(f"ğŸ¯ {label} (ä¿¡é ¼åº¦: {score:.2%})")
        
        # æ¤œå‡ºçµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
        if detections:
            result_text = f"âœ… {len(detections)}å€‹ã®ç‰©ä½“ã‚’æ¤œå‡ºã—ã¾ã—ãŸ:\n\n" + "\n".join(detections)
        else:
            result_text = "âš ï¸ ç‰©ä½“ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ä¿¡é ¼åº¦ã®é–¾å€¤ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚"
        
        return img_with_boxes, result_text
        
    except Exception as e:
        return image, f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}"

def classify_image(image, top_k):
    """ç”»åƒã‚’åˆ†é¡ã—ã¦ä¸Šä½Kå€‹ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™"""
    if image_classifier is None:
        return "âš ï¸ ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    
    if image is None:
        return "ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    
    try:
        # ç”»åƒåˆ†é¡ã‚’å®Ÿè¡Œ
        results = image_classifier(image, top_k=top_k)
        
        # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        output = "ğŸ·ï¸ **ç”»åƒåˆ†é¡çµæœ**\n\n"
        for idx, result in enumerate(results, 1):
            label = result['label']
            score = result['score']
            bar = "â–ˆ" * int(score * 20)
            output += f"{idx}. **{label}**\n"
            output += f"   {bar} {score:.2%}\n\n"
        
        return output
        
    except Exception as e:
        return f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}"

def custom_classify(image, custom_labels):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®ãƒ©ãƒ™ãƒ«ã§ç”»åƒã‚’åˆ†é¡ï¼ˆã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆï¼‰"""
    if zero_shot_classifier is None:
        return "âš ï¸ ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    
    if image is None:
        return "ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    
    if not custom_labels:
        return "âš ï¸ ãƒ©ãƒ™ãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"
    
    try:
        # ãƒ©ãƒ™ãƒ«ã‚’åˆ†å‰²
        labels = [label.strip() for label in custom_labels.split(",")]
        
        # ç”»åƒã®èª¬æ˜ã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if image_classifier:
            top_result = image_classifier(image, top_k=1)[0]
            image_description = f"An image containing {top_result['label']}"
        else:
            image_description = "An image"
        
        # ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡
        results = zero_shot_classifier(image_description, labels)
        
        # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        output = "ğŸ¯ **ã‚«ã‚¹ã‚¿ãƒ åˆ†é¡çµæœ**\n\n"
        for label, score in zip(results['labels'], results['scores']):
            bar = "â–ˆ" * int(score * 20)
            output += f"**{label}**\n"
            output += f"{bar} {score:.2%}\n\n"
        
        return output
        
    except Exception as e:
        return f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}"

def apply_ai_filter(image, filter_type, intensity):
    """AIã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç”»åƒã«é©ç”¨"""
    if image is None:
        return None
    
    try:
        img = image.copy()
        
        if filter_type == "ã‚¨ãƒƒã‚¸æ¤œå‡º":
            # OpenCVã§ã‚¨ãƒƒã‚¸æ¤œå‡º
            img_array = np.array(img)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            edges = cv2.Canny(gray, 50 * intensity, 150 * intensity)
            edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
            img = Image.fromarray(edges_colored)
            
        elif filter_type == "ã‚¹ã‚¿ã‚¤ãƒ©ã‚¤ã‚º":
            # è¼ªéƒ­å¼·èª¿
            img_array = np.array(img)
            stylized = cv2.stylization(img_array, sigma_s=60, sigma_r=0.07 * intensity)
            img = Image.fromarray(stylized)
            
        elif filter_type == "ã‚«ãƒ¼ãƒˆã‚¥ãƒ¼ãƒ³":
            # ã‚«ãƒ¼ãƒˆã‚¥ãƒ¼ãƒ³åŠ¹æœ
            img_array = np.array(img)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            gray = cv2.medianBlur(gray, 5)
            edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                         cv2.THRESH_BINARY, 9, 9)
            color = cv2.bilateralFilter(img_array, 9, 250, 250)
            cartoon = cv2.bitwise_and(color, color, mask=edges)
            img = Image.fromarray(cartoon)
            
        elif filter_type == "ãƒã‚ªãƒ³":
            # ãƒã‚ªãƒ³åŠ¹æœ
            enhancer = ImageEnhance.Contrast(img)
            img = enhancer.enhance(2.0 * intensity)
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(2.0 * intensity)
            
        elif filter_type == "ã‚µãƒ¼ãƒãƒ«":
            # ã‚µãƒ¼ãƒãƒ«ãƒ“ã‚¸ãƒ§ãƒ³é¢¨
            img_array = np.array(img)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            thermal = cv2.applyColorMap(gray, cv2.COLORMAP_JET)
            img = Image.fromarray(cv2.cvtColor(thermal, cv2.COLOR_BGR2RGB))
        
        return img
        
    except Exception as e:
        print(f"Filter error: {e}")
        return image

def generate_analysis_report(image, detections, classifications):
    """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
ğŸ“Š **AIç”»åƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â° åˆ†ææ™‚åˆ»: {timestamp}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### ï¿½ ç‰©ä½“æ¤œå‡ºçµæœ
{detections}

### ï¿½ï¸ ç”»åƒåˆ†é¡
{classifications}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– Powered by Transformer AI Models
- DETR (DEtection TRansformer)
- Vision Transformer (ViT)
- BART Zero-Shot Classifier
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    
    return report

# Gradio UIæ§‹ç¯‰
with gr.Blocks(theme=gr.themes.Soft(), title="ğŸ” AI Vision Lab") as demo:
    
    gr.Markdown("""
    # ğŸ” AI Vision Lab - æœ€å…ˆç«¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ä½“é¨“
    
    **æœ€æ–°ã®Transformerãƒ™ãƒ¼ã‚¹AIã§ç”»åƒã‚’è§£æï¼**
    - ğŸ¯ **DETR** - Facebookè£½ã®æœ€æ–°ç‰©ä½“æ¤œå‡ºAI
    - ğŸ‘ï¸ **Vision Transformer** - Googleè£½ã®ç”»åƒåˆ†é¡AI
    - ğŸ¨ **AI Filters** - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒå‡¦ç†
    - ğŸ§  **Zero-Shot Classification** - ã‚«ã‚¹ã‚¿ãƒ ãƒ©ãƒ™ãƒ«ã§åˆ†é¡
    
    ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€AIã®åŠ›ã‚’ä½“é¨“ã—ã‚ˆã†ï¼âœ¨
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ›ï¸ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«")
            
            text_prompt = gr.Textbox(
                label="ğŸ’¬ å‰µä½œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                placeholder="ä¾‹: æœªæ¥éƒ½å¸‚ã®å¤œæ™¯ã€ãƒ­ãƒœãƒƒãƒˆã¨äººé–“ã®å…±å­˜ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®å¤¢...",
                lines=3,
                value="äººå·¥çŸ¥èƒ½ãŒå‰µé€ æ€§ã‚’æŒã¤æœªæ¥"
            )
            
            with gr.Row():
                image_style = gr.Radio(
                    choices=["æŠ½è±¡", "å¹¾ä½•å­¦", "ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é¢¨", "ã‚°ãƒªãƒƒãƒ"],
                    value="ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é¢¨",
                    label="ğŸ¨ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«"
                )
                
                color_scheme = gr.Radio(
                    choices=["ã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯", "ãƒã‚ªãƒ³", "ãƒ‘ã‚¹ãƒ†ãƒ«", "ãƒ€ãƒ¼ã‚¯", "ãƒ“ãƒ“ãƒƒãƒ‰"],
                    value="ãƒã‚ªãƒ³",
                    label="ğŸŒˆ ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ"
                )
            
            text_temp = gr.Slider(
                minimum=0.1,
                maximum=2.0,
                value=0.8,
                step=0.1,
                label="ï¿½ï¸ å‰µé€ æ€§æ¸©åº¦",
                info="é«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ãƒ»å‰µé€ çš„"
            )
            
            text_length = gr.Slider(
                minimum=50,
                maximum=200,
                value=100,
                step=10,
                label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·",
                info="ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§é•·"
            )
            
            generate_btn = gr.Button(
                "ğŸš€ AIå‰µä½œã‚’é–‹å§‹",
                variant="primary",
                size="lg"
            )
            
            gr.Markdown("""
            ---
            ### ğŸ’¡ ãƒ’ãƒ³ãƒˆ
            - **å‰µé€ æ€§æ¸©åº¦**ã‚’ä¸Šã’ã‚‹ã¨äºˆæ¸¬ä¸å¯èƒ½ãªçµæœã«
            - **ã‚°ãƒªãƒƒãƒ**ã‚¹ã‚¿ã‚¤ãƒ«ã§ã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯æ„Ÿã‚’
            - æŠ½è±¡çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã»ã©é¢ç™½ã„çµæœã«ï¼
            """)
    
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ­ AIç”Ÿæˆçµæœ")
            
            with gr.Tab("ğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ"):
                text_output = gr.Textbox(
                    label="AIãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ",
                    lines=10,
                    max_lines=15
                )
                
                sentiment_output = gr.Textbox(
                    label="ğŸ§  AIæ„Ÿæƒ…åˆ†æ",
                    lines=2
                )
            
            with gr.Tab("ğŸ¨ ç”Ÿæˆã‚¢ãƒ¼ãƒˆ"):
                image_output = gr.Image(
                    label="AIãŒç”Ÿæˆã—ãŸãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¢ãƒ¼ãƒˆ",
                    type="pil"
                )
            
            with gr.Tab("ğŸ“Š å‰µä½œãƒ¬ãƒãƒ¼ãƒˆ"):
                report_output = gr.Textbox(
                    label="çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ",
                    lines=12
                )
    


if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0")
